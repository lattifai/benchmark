ChatGPT与谷歌AI大战升级：多模态之战

## Table of Contents
* [00:00:00] Intro
* [00:01:15] OPENAI GPT 4o
* [00:08:13] 谷歌的战书
* [00:13:33] 多模态之战与AI应用落地
* [00:19:48] 苹果加入战局？

## [00:00:00] Intro

ChatGPT以及硅谷AI大战终于升级，长出了眼睛和嘴。 [00:00:06]

五月中旬，OpenAI和谷歌前后发布重磅AI多模态更新，从基于文字交互的ChatGPT全面升级，实现了声音、文字和视觉三者全面结合的人工智能新交互功能。而这也标志着硅谷科技巨头们的生成式AI之战正式进入到第二轮。 [00:00:27]

新一轮的竞争只会更加激烈，更加全面。 [00:00:30]

[GPT-4o video clip: Hello, I'm here.] [00:00:34]

**陈茜:** Hello大家好，欢迎来到硅谷101，我是陈茜。我本来在希腊休假哈，但是因为OpenAI和谷歌的重磅更新啊，又被抓来加班更新了这期视频。来聊聊这次多模态AI之战，对科技巨头们的商业版图意味着什么变化，以及生成式AI智能技术的下一步会发生什么。那我们就首先来快速复盘一下OpenAI和谷歌发布的多模态重磅更新。 [00:01:00]

如果大家看过了发布会的话，可以直接跳到第三章的这个时间点，直接看分析。但没看过的，我们还是来回顾一下这场发布会，里面有很多细节还是挺重要的。 [00:01:13]

**Mira Murati:** Let's get started. [00:01:15]


## [00:01:15] OPENAI GPT 4o

**陈茜:** OpenAI这次的发布会时长很短，全程就26分钟，就发了一款产品GPT-4o。 [00:01:23]

GPT-4o的“o”是拉丁词根“Omni”，意思是所有的、全部的或者全能。那意味着文本、音频和图像的任意组合作为输入，并且生成文本、音频和图像输出的全面多模态能力。 [00:01:41]

说实话哈，2024年AI之战会升级到多模态产品，这个预期在2023年已经是行业共识。我们在之前的多期视频都提到过，仅仅是文字的prompt（指令）很难表达人类的意图，非常低效也非常受限。 [00:01:57]

所以有语音和视觉加持的多模态AI交互，是人类通往AGI道路上的必经之路。 [00:02:03]

但当多模态AI交互真的到来的时候，我觉得还是会被震撼到。 [00:02:08]

OpenAI说，GPT-4o可以在232毫秒内响应音频输入，平均为320毫秒。那这已经达到人与人之间响应的时间了。也就是说，AI语音对话的交互已经能够做到非常低延迟，很丝滑地像真人一样对话了。 [00:02:26]

那么GPT-4o发布之前，ChatGPT的语音模式功能有着好几秒的延迟，这让整个交互体验非常差。 [00:02:34]

这是因为之前的GPT系列的语音功能是好几个模型的拼合。先把声音转录成文本，再用GPT大模型接受后输出文本，然后再用text to speech（文字到语音）等等这样的模型去生成音频。但这其中会损失非常多的信息，比如说话语调、语气中的情绪情感、多个人说话人的识别、背景的声音等等。所以这个语音功能啊，之前很慢、很迟缓也很基础。 [00:03:02]

而这次，GPT-4o是OpenAI专门训练的跨文本、语音和视觉的端到端新模型。所有的输入和输出都是由同一个神经网络处理。那么这使得GPT-4o能够接受文本、音频和图像的任意组合作为输入，并生成文本、音频和图像的任意组合输出。是兼具了听觉、视觉的多模态模型。同时还支持中途打断和对话插入，并具备上下文记忆的能力。 [00:03:32]

这样的多模态模型是OpenAI首次发布，表示还有很多探索的空间。但目前展现出的功能已经让人惊喜了。 [00:03:40]

比如说在现场demo中，GPT-4o可以理解人们的呼吸急促声音，并用轻松的方式安慰人类。 [00:03:48]

**Mark Chen:** Okay, here I go. [Breathing heavily] [00:03:52]

**GPT-4o:** Whoa, slow down a bit there. Mark, you're not a vacuum cleaner. [00:04:00]

**Mark Chen:** Okay, I'll try again. Breathing in. And breathe out. [Breathing out] [00:04:05]

**GPT-4o:** That's it. How do you feel? [00:04:07]

**Mark Chen:** I feel a lot better. Thank you so much. [00:04:09]

**陈茜:** 它识别可以识别别人脸表情以及辨认情绪。 [00:04:12]

**GPT-4o:** Ah, there we go. It looks like you're feeling pretty happy and cheerful. [00:04:17]

**陈茜:** 它可以随意变换语气和风格来讲故事。 [00:04:21]

**Mark Chen:** Can you do this in a robotic voice now? [00:04:23]

**GPT-4o:** Initiating dramatic robotic voice. [00:04:27]

**陈茜:** 同时，GPT-4o还可以通过硬件设备，通过视觉，来分析人们正在从事的工作、看的书。可以引导人们解题，可以切换语言实时翻译，也能通过视觉识别给它的信息，并且给出非常拟人化的反馈。 [00:04:42]

**Barret Zoph:** Okay. So this is what I wrote down. What do you see? [00:04:45]

**GPT-4o:** Oh, I see "I love ChatGPT". That's so sweet of you. [00:04:51]

**陈茜:** 说实话啊，在直播发布会中，直接现场演示这件事情是很需要勇气的。因为一旦出错会引发非常大的公关灾难。但OpenAI有这个勇气去直接现场演示直播，给人的感觉非常自信。 [00:05:07]

那么除了现场的演示之外，OpenAI还在官网上放出了更多更复杂场景的交互，展现出AI多模态的更多的潜力。 [00:05:16]

比如说在官网上OpenAI做了17个案例展示，包括了照片转漫画、3D物体合成、海报创作、角色设计等等样本。 [00:05:25]

此外OpenAI总裁Greg Brockman的演示视频中，GPT-4o可以识别出他所穿的衣服、身处的环境，可以识别出Brockman的情绪和语气，还有房间里面正在出现的新动作。但最让外界关注的一个动作是，让两台运行GPT-4o的设备进行语音或者视频交互。 [00:05:47]

**Greg Brockman:** Do do do the singing voice again please. [00:05:50]

**GPT-4o (Device 1):** In a room where modern lights peak... [00:05:55]

**GPT-4o (Device 2):** Surprise guest where the playful streak... [00:06:01]

**陈茜:** 也就是说，OpenAI的GPT-4o多模态给了AI交互的声音和视觉，不仅升级了人和AI之间的交互，也升级了AI和AI之间的交互。那这样的交互更自然、更拟人，有着更大空间的应用场景。而且整个AI的声音和语言非常的灵动，机器人感比较弱。它会开玩笑、会安慰人、会害羞。难怪很多人在OpenAI发布会之后直呼，那部讲述人类和AI语音助手Samantha电影《她》的时代真的到来了。 [00:06:36]

**戴雨森:** 我自己是非常激动的啊。因为我一直觉得我们对于AI落地的很多应用预期其实不一定是准确的。大家可能在AI一开始的时候觉得生产力的场景也很直接。 [00:06:47]

但是现在可能发现好像说，这个理性的工作其实很多Agent（人工智能体）的落地啊什么反而比较难。但是感性的角度反而会更加容易一点。 [00:06:58]

第二呢我在朋友圈也发了我的感叹，就是说历史上当一件事情从很稀缺大家去抢，变得不稀缺的时候，可能就会带来巨大的变化。 [00:07:09]

对于绝大部分人来讲，生活其实是单调的，或者是一成不变的，是乏味的。那这个时候其实不管像《她》里面说所谓的这种男女情感的这个表达，还是说一种陪伴，还是说一种这个倾听，其实都是很多时候很稀缺的一种资源，或者一种内容。 [00:07:28]

那么当AI能做到，以一个低延迟、低成本，然后很好的形式去表达这种情绪价值的时候，这可能会对我们的社交、社会带来很大的影响，也会带来很大的这个机会。 [00:07:45]

**陈茜:** 随着AI能力的提升啊，图灵测试这个概念呢可能会越来越模糊化。电影《她》中描述的场景实现几乎是早晚的事。 [00:07:54]

但AI多模态带来的不仅仅是情感上的陪伴和交互，更多的是整个工作场景和生态上的颠覆。 [00:08:02]

就在OpenAI发布会的一天之后，谷歌发布的一系列多模态更新，进一步地说明了AI多模态能带来的颠覆性潜力。 [00:08:13]


## [00:08:13] 谷歌的战书

**陈茜:** 那比起OpenAI的发布会，谷歌的发布会就会更像一个巨头了。长达两个小时，在它的各个生态方向用AI去发力。连CEO Sundar Pichai自己也说，整场Keynote的演讲稿里总共提了120次“AI”，表明谷歌目前所有的工作都是围绕多模态AI模型Gemini来展开。 [00:08:37]

首先，直接与OpenAI前一天发布的GPT-4o对标的是Project Astra。 [00:08:42]

虽然谷歌不是现场演示，不像OpenAI那么敢啊，毕竟巨头还是需要保守一些些的。但是从谷歌的demo视频来看，如果谷歌的demo是实时生成的，谷歌的Gemini多模态模型比起OpenAI在功能上也不算弱。 [00:09:00]

谷歌DeepMind负责人Demis Hassabis在台上宣布了Project Astra。 [00:09:05]

Project Astra基于Gemini多模态大模型，是一个实时、多模态的人工智能助手。可以通过硬件设备“看到”世界，知道东西是什么，以及你把它们放在哪里。并且可以回答问题或帮助你做几乎任何事情。 [00:09:21]

在谷歌的demo视频中，谷歌伦敦办事处的一名工作人员用Astra识别自己的地理位置，找到丢失的眼镜，检查代码等等。 [00:09:30]

**Google Staff:** What can I add here to make this system faster? [00:09:35]

**Project Astra:** Adding a cache between the server and database could improve speed. [00:09:39]

**陈茜:** 如果谷歌demo是实时拍摄的，反正Demis Hassabis是打包票说，这个视频没有任何篡改。那么毫无疑问这会解锁众多的交互场景。 [00:09:50]

Hassabis说，展望未来，人工智能的故事将不再是关于模型本身，而是关于它们能为你做什么。 [00:09:59]

而与OpenAI的GPT-4o宣战的Project Astra，只是其中的一个产品而已。谷歌其实发布了非常多的更新，包括谷歌展示了最新版Gemini加持的搜索功能。 [00:10:12]

谷歌首先在美国上线名为AI Overviews的AI技术生成摘要功能。 [00:10:18]

简单来说啊，在你搜索信息的时候，谷歌的AI就能直接帮你查找、整理和展示了。具体来说，通过多步推理，Gemini可以代替用户研究，实现更好更高效的搜索总结和结果。 [00:10:31]

比如说规划一日三餐、购物餐厅选择、行程规划都可以在AI搜索中完成。重要的是，这样的AI搜索还会直接帮你做规划。比如说“帮我创建一个3天的饮食计划”，谷歌AI搜索就直接一个计划书摆在你面前了。 [00:10:47]

另外让我觉得很期待的两个功能，一个就是多模态搜索了。你会不会遇到过这种情况，搜索时发现难以用语言描述问题，或者遇到不熟悉不认识的物体，不知道如何去搜索相关的名词。 [00:11:02]

现在你就可以直接拍张照片或者录段视频，用语音或打字问AI搜索，这个是啥、怎么修理。之后谷歌就会帮你整理出相关的各种信息。 [00:11:14]

**Rose Yao:** Why will this not stay in place? [00:11:18]

**Rose Yao:** And in a near instant, Google gives me an AI Overview. [00:11:23]

**陈茜:** 对于我这种3C杀手，经常容易弄坏各种电器的人来说，我简直太期待这个多模态搜索功能了。而多模态模型Gemini的强大搜索和推理能力还能做更多的事情，也正好是我的痛点。 [00:11:39]

比如说CEO Pichai在现场演示，Gemini可以在谷歌相册Google Photos里进行更多的相关搜索。比如通过名为Ask Photos with Gemini的新功能，让Gemini找到用户想要的车牌照号。 [00:11:53]

实话告诉大家，我就是那个记不住我家车牌号的人。如果大家也这样，可以弹幕让我知道我不是一个人。 [00:12:01]

所以，谷歌Gemini可以在用户的相册中搜索，找到相应信息和对应的照片。来比如说获取照片中拍到的车牌照号码。这个功能对我来说，真的是非常期待。 [00:12:14]

以及任何可以帮我寻找，以往照片、文件中信息的功能，我觉得都会解决很多痛点。 [00:12:20]

还有一个对我来说很大帮助的是，谷歌AI将会结合到谷歌的所有workspace中，俗称“谷歌全家桶”。也就是说在Gemini的加持下，Google Workspace，包括Gmail, Google Docs, Google Drive, Google Calendar, Google Meet等等都可以打通。可以在这里进行跨文档搜索。 [00:12:39]

比如说你在邮箱里收到了一张发票，那么可以直接通过Gemini把这张发票整理到网盘Google Drive和表格Google Sheet中。还可以在邮件中搜索，读取信息和亮点，归纳总结。这些功能都会在今年稍后推出。 [00:12:55]

另外谷歌还发布了一系列其他的模型更新，包括画图的Imagen 3，音乐的Music AI Sandbox，还有生成视频的Veo，还有有史以来最长上下文窗口200万token的Gemini 1.5 Pro，还有Gemini app，以及谷歌的自研芯片第六代TPU等等。 [00:13:14]

因为细节和产品太多了，这个视频我们就不一一复述了。如果感兴趣的小伙伴可以去看谷歌的两小时发布会全程。 [00:13:23]

看到这里你可能会问，在OpenAI之后发布这一系列重磅更新的谷歌，两个对手这一轮的发布，谁是赢家呢？ [00:13:33]


## [00:13:33] 多模态之战与AI应用落地

**陈茜:** 两场发布会之后，我看到不少人在对比OpenAI和谷歌的产品发布。我们从公司策略层来解读一下。 [00:13:43]

首先OpenAI比谷歌IO早一天发布了春节更新，而且非常临时。很难猜测不是故意抢在谷歌前面的。发布时长也只持续26分钟，非常聚焦在GPT-4o这一个产品上。 [00:13:57]

虽然外界对GPT-4o的评价，没有说像当时发布ChatGPT时那么惊喜那么轰动，但不得不说，业内的很多人还是觉得是一个很重要的里程碑。 [00:14:08]

虽然多模态的这些功能是去年业内共识，OpenAI会在2024年做出来并发布，并没有那么多惊喜或创新。但是“实现”了大家“期待中早晚会实现的AI更新”，也是非常有意义的，并且也是正确的发展道路。 [00:14:25]

**Howie Xu:** 我觉得是，因为如果一个技术不能被用起来，这个技术背后再怎么样也没啥意思。OpenAI这个GPT-4模型出来，也能够做些translation（翻译），就翻译什么的并不是一个新东西。 [00:14:41]

但如果没有实时效应，其实是很难落地。但星期一他那个宣布的东西，让我感觉到我有可能会真的会去用。有可能在，比如下次我跟你一起去，不管是采访谁或者跟谁讲话，语言不通（的时候），我们真的可能就，打开我们的手机来给来用translation（翻译）。 [00:15:02]

否则的话就以前的那个延迟这么慢，那个效果很不好，你都不好意思拿出来就用对吧。 [00:15:09]

那为什么能够做到延迟性这么低？那被广泛认为的就是因为它是做了一个比较，就像你说的原生的Native的一个Multimodal（多模态模型）。就我看到那个demo，我的第一反应是说OK，从前他说的这些东西我都是玩玩是可以的，但是我是不会拎出来用的。但是他星期一给我的东西，我就觉得有可能我会拿来，就在实际的生活工作的场景里面可能用得到。 [00:15:35]

**陈茜:** 如果光从语音助手这个产品上来看，GPT-4o对打谷歌Project Astra，目前业内很多声音仍然后认为是OpenAI是领先的。 [00:15:45]

单从多模态模型上来说，GPT-4o是OpenAI第一款完全原生的多模态模型。我们视频之前也说到，它所有的多模态输入和输出都由同一个神经网络处理。这使得GPT-4o能够接受文本、音频和图像的任意组合作为输入，并生成文本、音频和图像的任意组合输出，是所谓的“multimodal in, multimodal out”（多模态输入，多模态输出）。 [00:16:09]

但目前不少业内人士认为，谷歌的Gemini目前并没有做到这个程度。比如说英伟达高级科学家Jim Fan，他在LinkedIn上发表观点认为，谷歌是多模态作为输入，但并不是多模态作为输出。multimodal in but not multimodal out。 [00:16:29]

这意味着谷歌本次更新的视频、音乐等模型，依然是独立于Gemini大模型的存在。只是输出的时候，把所有模型给整合起来拥有的多模态输出能力。所以Jim Fan认为，谷歌整合所有的输入输出模态，将是不可避免的未来发展。 [00:16:45]

但他还有一句评论挺有意思的。Jim Fan说，谷歌在这次发布会中做对的一件事是，他们终于认真努力将AI集成到搜索框中。谷歌最坚固的护城河是分销。Gemini不一定要成为最好的模型，才能成为世界上被使用最多的模型。 [00:17:04]

也就是说，谷歌在整个生态中只要顺畅地融入AI功能，让用户觉得能解决问题，提高生活和工作效率。因为谷歌在搜索、邮箱、谷歌云上的种种积累和优势，谷歌的分销优势依然能保证谷歌在AI时代中立于不败之地。 [00:17:23]

所以，按照这个逻辑来看，谷歌在这次发布会上，在全生态上全面升级AI功能，其实是做到了。所以就算OpenAI前一天抢跑发布亮点的GPT-4o，谷歌整体来看，这一局也不算输。第二天的股价稳中上涨也应证了市场的看法。 [00:17:44]

**戴雨森:** 在OpenAI发布会之后，Google发布会之前，我跟一位Google的同学聊，然后他提到一个观点还挺有意思。他说一年以前OpenAI发GPT-4的时候，他们有很多东西，他们是不知道OpenAI怎么做到的，觉得哇他们好厉害，对吧。 [00:18:00]

现在在OpenAI发布会发了之后，他们看到是说，噢这个东西我们也知道怎么做，但我们可能还没有像他那样做得那么好，或者那么ready（准备好）去demo。 [00:18:08]

所以我觉得目前来看的话，他们肯定在这上面是有一些这个经验。所以我感觉就是双方的绝对差距还是在缩小的。 [00:18:16]

**Howie Xu:** 相对来讲，Google注重的是一个solution（解决方案），就是解决和方案。那个OpenAI目前注重的更多的还是一个technology（技术）。它在technology上面非常的惊艳，但你说他怎么去跟我们的日常，不管是生活、工作去结合起来，他没有那么多的人力，他也没这么多思考，而且这不是他的强项。 [00:18:37]

Google IO的那个发布，看上去可能从某些角度来讲，好像还没那个前一天的OpenAI的东西那么惊艳。但实际上我觉得很惊艳。我觉得惊艳不只是说是一个model（模型）的惊艳，model只是一个维度，还有其他维度。怎么跟我的生活、工作能结合起来，比如跟我手机结合起来，它一些的announcement（发布）是这个技术。 [00:19:02]

所以说AI这件技术，我觉得今天落地是一个很很大的一个挑战，或者说一件事情。 [00:19:09]

**陈茜:** 所以可以预期到，多模态的继续整合和优化，以及将AI功能整合到谷歌的各个产品中，以及AI agent（人工智能体）的引入，将会是谷歌发力的重点。 [00:19:20]

除此之外，这两场发布会听下来，还让我非常感兴趣的一点是：硬件。 [00:19:26]

OpenAI整个demo用的是苹果手机和苹果电脑。谷歌用的是安卓手机和硬件。同时还在视频demo中提到了一个谷歌内部类似谷歌眼镜一样的prototype原型设备。 [00:19:39]

所以接下来，硬件和AI大模型的整合，也到了入场战场的时刻。而这个赛道的老大，苹果，在干什么呢？ [00:19:48]


## [00:19:48] 苹果加入战局？

**陈茜:** 虽然苹果公司在这轮硅谷科技巨头AI大战中，迟迟没有发声。但最近不少的舆论风向稍微给我们勾勒出了苹果潜在的想法和布局。 [00:20:01]

目前市场都在等待6月10号举行的苹果2024年全球开发者大会WWDC。预计会在届时，会宣布一系列在AI和硬件上的产品发布。包括可能会和OpenAI合作，将ChatGPT整合到iOS 18操作系统。 [00:20:17]

另外外界期待苹果会宣布，利用大模型全面升级Siri，给用户提供AI赋能的交互体验。还有苹果如何将大模型塞进手机移动端的“苹果全家桶”，也是马上召开的苹果发布会的最大看点。 [00:20:35]

那么今年早前，苹果是发布了一系列的论文，包括第一个手机端UI多模态大模型Ferret-UI。还有今年一月发布的，一篇将大模型塞进iPhone的关键性论文，“使用有限的内存实现更快的LLM推理”。 [00:20:50]

还有这篇，苹果Siri团队在论文《利用大型语言模型进行设备指向性语音检测的多模态方法》中，讨论了去掉唤醒词的方法。 [00:21:00]

同时在今年3月发布的另一篇论文中，苹果首次披露一个具有高达300亿参数的多模态模型MM1。那么这个多模态能力如果集成到iPhone上，就能通过视觉、语音和文本等多种方式，理解并响应用户的需求。 [00:21:18]

**陈茜:** 所以综上所述，虽然最近两年来，苹果时常被人诟病在AI领域动作迟缓，但是感觉苹果是在等一个正确的时机来加入战局。它并没有落后，而是一直在等待。 [00:21:31]

如今，多模态技术成熟，特别是文字输入、语音和视觉的交互，和手机等硬件是天然的适配。OpenAI和谷歌的AI多模态之战打响之际，也是苹果入局的时间了。 [00:21:45]

**戴雨森:** 如果你看互联网和移动互联网时代，其实他们在软件的渗透上，其实都要叠加一个硬件的渗透。大家要买PC，大家要买手机。所以导致说，之前一个软件它的渗透速度，其实是相对比较慢的。 [00:21:58]

那为什么比如ChatGPT，一出来就渗透到了这么多的用户，实际是因为它跑在一个已有比较成熟的硬件上。所以我觉得在目前来讲，AI应用落地肯定首选还是在手机上。 [00:22:08]

我觉得肯定是期待像AI的这些模型，怎么样在苹果的生态系统中去落地。其实说全新形态的硬件，我自己觉得可能性比较低。但是在在这个上面有，包括最近刚发M4对吧。大家说这个上面有这么强的这个芯片，你如果还是做原来的任务，是不是就浪费了。你是不是应该来干一些AI的任务呢。 [00:22:25]

其实我觉得如果你去看两年前，两年前一些几百b（几千亿参数），上百b（上千亿参数）的模模型的能力，现在可能到了几十b（几百亿参数），甚至在这个十几b（一百多亿参数）的这个情况下，都能够去进行同样的性能。这个其实是一个非常好的一个过程。 [00:22:43]

**陈茜:** 而对于智能手机、智能手表，乃至于以后的VR和AR眼镜设备，更小的端模型将是业界着重发力的重点。 [00:22:52]

在今年4月，苹果宣布在全球最大AI开源社区Hugging Face上发布了全新的开源大型语言模型OpenELM系列模型。包括4个不同参数规模的模型，270 Million（百万），450 Million（百万），1.1 Billion（十亿）和3 Billion（十亿）。 [00:23:07]

没错，最大的也只有30亿个参数。对移动端小模型的布局有着明显的意图。 [00:23:13]

而Howie Xu在采访中认为，端模型是人类应用AI发展的必然趋势。 [00:23:19]

**Howie Xu:** 过去一年我们的大量的energy（精力），大量的讨论都是在越大越好，对吧。但是万亿级的parameter（参数）不适合放在手机上面。 [00:23:29]

那另外一个问题就是说，那个不是万亿级的对吧，千亿级的，或者说是百亿级的参数，是不是能够把模型做到足够好。 [00:23:40]

现在我们看到的很多的小的模型可能是700亿参数的。一年之内我们能看到，十亿这么一个数量级的一个model（模型），能做到，2022年11月30号它第一次出来时候，让大家惊艳的那个，相当于（GPT）3.5的那个model（模型）的能力。 [00:24:00]

我觉得是一个billion（十亿）的parameter（参数）是应该能够做到。如果能在这个端上面能够run（运行）一个billion parameter的model（十亿参数级别的模型），能做到（GPT）3.5的（能力），那就打开了很多的想象空间。 [00:24:15]

然后接下去会有更小的model（模型），因为model总是越小，对耗电、对各方面的都有很大好处。我觉得甚至是sub 1 billion（小于10亿参数）的会更好。从privacy（隐私）的角度，从耗电的角度，从各方面角度，我觉得小模型是必须的。 [00:24:34]

**陈茜:** 视频的最后，我们来总结一下OpenAI和谷歌的这两场发布会。 [00:24:40]

AI多模态之战打响之后，在更多更广的应用上，我们看到了AI杀手级应用的曙光。有了更落地更切实的可用性。这将重塑人类和AI以及电子设备的交互方式。 [00:24:53]

此外，虽然OpenAI和谷歌表面上刀光剑影，但两家公司的策略目标是有一些区别的。前者一路勇向向前目标scaling law（规模法则）和AGI，后者更注重自家生态和应用落地，来捍卫商业营收与市场分销护城河。模型是不是最好的，并没有那么重要。 [00:25:13]

所以目前的多模态初战，OpenAI虽然赢了，但谷歌也没输。而在硬件端，各类硬件与AI的结合将带来巨大的新机会。而大模型“瘦身”进手机只是开始，打造应用体验才是关键所在。 [00:25:28]

另外，让人惊喜的是，谷歌demo最后展示的AR眼镜与AI的结合。这给“AR智能眼镜”这个起起伏伏了好几个周期的产品，带来了新的曙光和希望。 [00:25:39]

除了谷歌多年的AR经验，Meta在AR硬件上的布局，与苹果在Vision Pro以及自家AR团队的未来策略，都可能成为下一场科技硬件巨头们比拼的新战场。 [00:25:51]

哦对了，不要忘记微软这家与OpenAI深度绑定的巨头。它并没有将全部鸡蛋都放在OpenAI的篮子中。 [00:25:59]

微软目前在AI布局上的优势，加上在软硬件上都有多年经验和布局，最近还收编了之前主打情感陪伴大模型公司Inflection的大部分AI顶级人才，发布了自己的大模型MAI-1。 [00:26:14]

所以我们很兴奋地能感觉到，生成式AI的第二轮多模态战役打响了。越来越多的科技巨头入局，并且战术和方向也越发清晰。带来的也是AI应用的潜在落地与爆发。 [00:26:27]

这场战斗，硅谷101在最前线，所以别忘了关注我们的账号别错过更新。那我们就拭目以待。 [00:26:33]

以上就是这期内容的全部内容。抱歉我们这期背景有点不硅谷哈，因为我在希腊休假的时候被临时抓来加录的这一期。在这么平静原始的海岛上，看OpenAI和谷歌的直播，并且写了这篇AI的稿件，感觉非常穿越和有趣。 [00:26:52]

同时也感谢大家，欢迎大家留言、点赞、转发和评论。你们的支持是我们硅谷101做好深度科技内容和商业内容的最佳动力。我们下一个视频，硅谷再见啦。Bye bye。 [00:27:07]