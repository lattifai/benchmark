[Script Info]
; Script generated by pysubs2
; https://pypi.python.org/pypi/pysubs2
WrapStyle: 0
ScaledBorderAndShadow: yes
Collisions: Normal
ScriptType: v4.00+

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: Default,Arial,20,&H00FFFFFF,&H000000FF,&H00000000,&H00000000,0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
Dialogue: 0,0:00:01.80,0:00:06.52,Default,host,0,0,0,,ChatGPT以及硅谷AI大战终于升级，长出了眼睛和嘴。
Dialogue: 0,0:00:06.91,0:00:26.61,Default,host,0,0,0,,5月中旬，OpenAI和谷歌前后发布重磅AI多模态更新，从基于文字交互的ChaGPT全面升级实现了声音、文字和视觉三者全面结合的人工智能新交互功能，而这也标志着硅谷科技巨头们的生成式AI之战正式进入到第二轮。
Dialogue: 0,0:00:27.05,0:00:30.40,Default,host,0,0,0,,那么新一轮的竞争只会更加的激烈更加的全面。
Dialogue: 0,0:00:33.25,0:00:34.08,Default,ChatGPT0,0,0,0,,Hello, I'm here.
Dialogue: 0,0:00:34.08,0:00:36.98,Default,host,0,0,0,,Hello，大家好，欢迎来到硅谷101，我是陈倩。
Dialogue: 0,0:00:37.24,0:00:44.04,Default,host,0,0,0,,我本来在希腊休假，但是因为OpenAI和谷歌的重磅更新啊，又被抓来加班更新了这期的视频。
Dialogue: 0,0:00:44.23,0:00:54.24,Default,host,0,0,0,,来聊聊这次的多模式AI之战对于科技巨头们的商业版图意味着什么变化以及生成式AI智能技术的下一步会发生什么。
Dialogue: 0,0:00:54.45,0:01:00.04,Default,host,0,0,0,,那我们就首先来快速复盘一下OpenAI和谷歌发布的多模态重磅更新。
Dialogue: 0,0:01:00.35,0:01:13.55,Default,host,0,0,0,,那如果大家已经看过了发布会的话，可以直接跳到第三章的这个时间点，啊，直接看分析，但是没看过的，我们还是来回顾一下这场发布会，啊，里面有很多的细节还是挺重要的。
Dialogue: 0,0:01:13.98,0:01:14.83,Default,Mira Murati,0,0,0,,Let's get started.
Dialogue: 0,0:01:18.10,0:01:24.00,Default,host,0,0,0,,OpenAI这次的发布会时长很短，全程就26分钟，就发了一款产品GPT-4o。
Dialogue: 0,0:01:24.40,0:01:41.39,Default,host,0,0,0,,GPT-4o的o是拉丁词根Omni，意思是所有的、全部的或者是全能，那意味着文本、音频和图像的任意组合作为输入并且生成文本、音频和图像输出的全面多模态能力。
Dialogue: 0,0:01:41.45,0:02:08.79,Default,host,0,0,0,,说实话，2024年AI之战会升级到多模态产品，这个预期在2023年已经是行业共识了，我们在之前的多期视频当中啊，都已经提到过仅仅是文字的prompt很难表达人类的意图，非常的低效也非常的受限，所以呢，有语音和视觉加持的多模态AI交互是人类通往AGI的道路上必经之路，但是当多模态AI交互真的到来的时候，我还是感觉会被震撼到。
Dialogue: 0,0:02:09.34,0:02:26.61,Default,host,0,0,0,,OpenAI说啊，GPT 4o可以在232ms之内响应，音频输入平均是320ms，那这已经是达到人与人之间响应的时间了，也就是说啊，AI语音对话的交互已经能够做到非常低延迟、很丝滑地像真人一样对话了。
Dialogue: 0,0:02:27.06,0:02:48.49,Default,host,0,0,0,,那么GPT-4o发布之前，ChatGPT的语音模式功能有着好几秒的延迟，那么这让整个交互体验非常得差，这是因为之前的GPT系列的语音功能是好几个模型的一个拼合，它先把声音转录成文本然后再用GPT大模型去接受之后呢，再输出文本，然后再用Text-to-Speech等等这样的模型去生成音频。
Dialogue: 0,0:02:48.76,0:03:01.99,Default,host,0,0,0,,但这其中呢，就会损失非常多很重要的信息， 比如说，语调语气中的情感情绪啊，还有多个人说话人的识别啊、背景的声音等等，所以这个语音功能啊，之前很慢很迟缓也很基础。
Dialogue: 0,0:03:02.24,0:03:32.08,Default,host,0,0,0,,而这次GPT-4o呢，是OpenAI专门训练的跨文本语音和视觉的端到端新模型，所有的输入和输出呢都是由同一个神经网络处理，那么这使得GPT-4o能够接受文本、音频和图像的任意组合作为输入并且生成文本、音频和图像的任意组合的输出，那是兼具了听觉、视觉的多模态模型，同时呢还支持中途打断和对话的插入，并且呢，具备上下文记忆的能力。
Dialogue: 0,0:03:32.22,0:03:48.88,Default,host,0,0,0,,那么，这样的多模态模型是OpenAI首次发布，表示呐还有很多的探索空间啊，但是目前展出来的功能已经是非常让人惊喜的了。比如说，在现场demo当中，GPT-4可以理解人们呼吸急促的声音，并且用轻松的方式安慰人类。
Dialogue: 0,0:03:48.97,0:03:49.65,Default,Mark Chen,0,0,0,,Ok, here I go.
Dialogue: 0,0:03:52.84,0:03:53.72,Default,ChatGPT,0,0,0,,Wow, slow...
Dialogue: 0,0:03:56.17,0:03:56.97,Default,ChatGPT,0,0,0,,Little bit there...
Dialogue: 0,0:03:57.85,0:04:00.06,Default,ChatGPT,0,0,0,,Mark, you're not a vacuum cleaner.
Dialogue: 0,0:04:00.33,0:04:02.08,Default,Mark Chen,0,0,0,,Ok, I'll try again, breathing in...
Dialogue: 0,0:04:03.37,0:04:04.02,Default,Mark Chen,0,0,0,,And breath out...
Dialogue: 0,0:04:05.63,0:04:06.15,Default,ChatGPT,0,0,0,,That's it...
Dialogue: 0,0:04:06.81,0:04:07.51,Default,ChatGPT,0,0,0,,How do you feel?
Dialogue: 0,0:04:07.68,0:04:08.38,Default,Mark Chen,0,0,0,,I feel a lot better.
Dialogue: 0,0:04:08.68,0:04:09.18,Default,Mark Chen,0,0,0,,Thank you so much.
Dialogue: 0,0:04:09.27,0:04:12.33,Default,host,0,0,0,,它可以识别人脸表情，以及呢，辨别情绪。
Dialogue: 0,0:04:12.51,0:04:14.20,Default,ChatGPT,0,0,0,,Ah, there we go.
Dialogue: 0,0:04:14.60,0:04:17.64,Default,ChatGPT,0,0,0,,It looks like you're feeling pretty happy and cheerful .
Dialogue: 0,0:04:17.85,0:04:21.09,Default,host,0,0,0,,它可以随意变换语气和风格来讲故事。
Dialogue: 0,0:04:21.27,0:04:23.37,Default,Mira Murati,0,0,0,,Can you do this in a robotic voice now?
Dialogue: 0,0:04:24.10,0:04:27.19,Default,ChatGPT,0,0,0,,Initiating dramatic robotic voice.
Dialogue: 0,0:04:27.76,0:04:42.40,Default,host,0,0,0,,同时呢，GPT-4o还可以通过硬件设备，通过视觉，来分析人们正在从事的工作、看的书，可以引导人们解题、可以切换语言实时翻译，也能够通过视觉识别给它的信息并且给出非常拟人化的反馈
Dialogue: 0,0:04:42.57,0:04:43.94,Default,Barrett Zoph,0,0,0,,Ok, so this is what I wrote down.
Dialogue: 0,0:04:44.13,0:04:44.65,Default,Barrett Zoph,0,0,0,,What do you see?
Dialogue: 0,0:04:46.55,0:04:47.03,Default,ChatGPT,0,0,0,,Uh...
Dialogue: 0,0:04:47.49,0:04:51.79,Default,ChatGPT,0,0,0,,I see, "I love ChatGPT", that's so sweet of you!
Dialogue: 0,0:04:52.66,0:05:16.07,Default,host,0,0,0,,说实话啊，在直播发布会中直接现场演示这件事情啊，是非常需要勇气的，因为一旦出错呢，会引发非常大的公关灾难。但是OpenAI有这个勇气去直接现场演示直播，给人的感觉也是相当自信哈。那么，除了现场的演示之外呢，OpenAI还在官网上放出了更多、更加复杂场景的交互，展现出AI多模态的更多的潜力。
Dialogue: 0,0:05:16.46,0:05:39.48,Default,host,0,0,0,,比如说，在官网上OpenAI做了17个案例展示，包括了照片转漫画、3D物体合成、海报创作、角色设计等等样本。那么，此外在OpenAI总裁Greg Brockman的演示视频当中啊，GPT-4o可以识别出他所穿的衣服、身处的环境，可以识别出Brockman的情绪和语气，还有房间里面正在出现的新动作。
Dialogue: 0,0:05:39.79,0:05:47.48,Default,host,0,0,0,,但是，最让外界关注的一个动作是，他让两台运行GPT-4o的设备进行语音，或者视频交互。
Dialogue: 0,0:05:47.86,0:05:49.86,Default,User_1,0,0,0,,Do, do, do the thing in voice again, please.
Dialogue: 0,0:05:50.84,0:05:54.17,Default,ChatGPT,0,0,0,,In the room where modern lights peak...
Dialogue: 0,0:05:58.30,0:06:01.66,Default,ChatGPT,0,0,0,,Surprise guest where the playful sterric.
Dialogue: 0,0:06:01.86,0:06:15.55,Default,host,0,0,0,,也就是说，OpenAI的GPT-4o多模态给了AI交互的声音和视觉，不仅是升级了人和AI之间的交互，也升级了AI和AI之间的交互。那这样的交互更自然、更拟人，有着更大空间的应用场景。
Dialogue: 0,0:06:15.85,0:06:30.99,Default,host,0,0,0,,而且整个AI的声音和语言非常的灵动，机器人感比较弱，它会开玩笑、会安慰人、会害羞，难怪很多人在OpenAI发布会之后直呼那部讲述人类和AI语音助手Samantha电影《Her》的时代真的到来了！
Dialogue: 0,0:06:31.50,0:06:32.58,Default,ChatGPT,0,0,0,,OK, and stop.
Dialogue: 0,0:06:34.08,0:06:34.79,Default,ChatGPT,0,0,0,,Walk forward.
Dialogue: 0,0:06:35.92,0:06:36.18,Default,ChatGPT,0,0,0,,And...
Dialogue: 0,0:06:36.07,0:06:58.46,Default,Yusen Dai,0,0,0,,我自己是非常激动的，呃，因为，呃，我一直觉得，我们对于AI落地的很多应用预期其实不一定是准确的。大家可能在AI一开始的时候觉得生产力的产品很直接，呃，但是现在可能发现好像说这个理性的工作，其实，呃，很多agent落地啊什么反而比较难，但是感性的角度反而会更加容易一点。
Dialogue: 0,0:06:58.57,0:07:09.27,Default,Yusen Dai,0,0,0,,第二呢，我在朋友圈也发了一波的感叹，就是说，嗯，历史上当一件事情从很稀缺，大家抢，变得不稀缺的时候，可能就会带来巨大的变化。
Dialogue: 0,0:07:09.58,0:07:28.12,Default,Yusen Dai,0,0,0,,对于主要朋友来讲，生活其实是，嗯，单调的，或者是一尘不变的、是乏味的。啊，那这个时候，其实不管像《Her》里面所谓的这种，呃，男女情感的表达，还是说一种陪伴，还是说一种这个倾听，其实都是，很多时候很稀缺的一种资源、或者一种呃，内容。
Dialogue: 0,0:07:28.56,0:07:45.23,Default,Yusen Dai,0,0,0,,啊，那么，呃，当AI能够做到，呃，以一个低延迟、低成本，呃，然后很好的形式去表达这种情绪价值的时候，这可能会对我们的社交啊、社会呀，带来很大的影响，会带来，带来很大的利润机会。
Dialogue: 0,0:07:45.75,0:07:53.96,Default,host,0,0,0,,随着AI能力的提升啊，图灵测试这个概念呢，可能会越来越模糊化。电影《Her》中描述的场景实现几乎是早晚的事儿。
Dialogue: 0,0:07:54.31,0:08:01.66,Default,host,0,0,0,,但AI多模态带来的不仅仅是情感上的陪伴和交互，更多的是整个工作场景和生态上的颠覆。
Dialogue: 0,0:08:02.75,0:08:12.33,Default,host,0,0,0,,那么就在OpenAI发布会的一天之后呢，谷歌发布的一系列多模态更新进一步地说明了AI多模态能够带来的颠覆性潜力。
Dialogue: 0,0:08:15.69,0:08:37.04,Default,host,0,0,0,,但比起OpenAI的发布会啊，谷歌的发布会呢就更像一个巨头了，长达两个小时，在它的各个生态的方向啊用AI去发力。连CEO，Sundar Pichai自己也说整场keynote的演讲稿件当中啊，总共提了120次的AI。表明呢，谷歌目前所有的工作都是围绕着多模态AI模型Gemini来展开。
Dialogue: 0,0:08:37.27,0:08:42.44,Default,host,0,0,0,,首先呢，直接与OpenAI前一天发布的GPD 4o对标的是Project Astra。
Dialogue: 0,0:08:42.82,0:08:59.61,Default,host,0,0,0,,虽然谷歌不是现场演示，不像OpenAI那么赶哈，毕竟巨头还是需要保守一些些的。但是从谷歌的Demo视频来看呢，如果谷歌的Demo是实时生成的，谷歌的Gemini多模态模型比起OpenAI在功能上啊也不算弱。
Dialogue: 0,0:09:00.16,0:09:05.00,Default,host,0,0,0,,谷歌DeepMind的负责人Demis Hassabis在台上宣布了Project Astra。
Dialogue: 0,0:09:05.41,0:09:20.78,Default,host,0,0,0,,那Project Astra呢，是基于Gemini多模态大模型，是一个实时多模态的人工智能助手，可以通过硬件设备看到世界，知道东西是什么，以及呢，你把它们放在哪里，并且可以回答问题，或者帮助你做几乎任何事情。
Dialogue: 0,0:09:21.22,0:09:30.49,Default,host,0,0,0,,在谷歌的demo视频当中啊，谷歌伦敦办事处的一个工作人员就用Astra识别自己的地理位置找到丢失的眼镜、检查代码等等。
Dialogue: 0,0:09:30.50,0:09:33.04,Default,Astra User1,0,0,0,,What can I add here to make the system faster?
Dialogue: 0,0:09:36.27,0:09:39.65,Default,ChatGPT,0,0,0,,Adding the cash between the server and database could improve speed.
Dialogue: 0,0:09:40.00,0:09:58.99,Default,host,0,0,0,,如果谷歌的DEMO是实时拍摄的，反正Demis Hassabis是打包票说这个视频没有任何的改动， 那么毫无疑问啊，就会解锁众多的互动场景。那么Hassabis说，展望未来，人工智能的故事将不再是关于模型本身，而是关于它们能够为你做什么。
Dialogue: 0,0:09:59.51,0:10:11.45,Default,host,0,0,0,,而与OpenAI的GPT-4o宣战的Project Astra只是其中的一个产品而已，谷歌呢其实发布了非常多的更新，包括谷歌展示了最新版Gemini加持的搜索功能。
Dialogue: 0,0:10:12.40,0:10:17.53,Default,host,0,0,0,,谷歌首先在美国上线了名为AI Overviews的AI技术生成摘要功能。
Dialogue: 0,0:10:18.07,0:10:23.84,Default,host,0,0,0,,简单来说啊，在你搜索信息的时候，谷歌的AI呢就能够直接帮你查找、整理和展示了。
Dialogue: 0,0:10:24.13,0:10:47.09,Default,host,0,0,0,,具体来说啊，通过多步推理，Gemini可以代替用户研究，实现更好的、高效的搜索总结和结果。比如说，规划一日三餐、购物餐厅选择、行程规划都可以在AI搜索中完成。更重要的是，这样的AI搜索还会直接帮你做规划，比如说，帮我创建一个三天的饮食计划，那谷歌AI搜索啊，就直接一个计划书摆在你面前了。
Dialogue: 0,0:10:47.56,0:11:01.93,Default,host,0,0,0,,另外让我觉得很期待的两个功能啊，第一个呢就是多模态搜索了。你会不会遇到过这种情况啊，搜索的时候发现难以用语言描述问题，或者遇到不熟悉、不认识的物体，不知道如何去搜索相关的名词？
Dialogue: 0,0:11:02.67,0:11:13.98,Default,host,0,0,0,,但现在呢，你就可以直接拍一张照片，或者呢录一段视频，用语音或者打字，问AI搜索：“这个是啥？怎么修理？”之后，谷歌呢，就会帮你整理出相关的各种信息。
Dialogue: 0,0:11:14.32,0:11:17.81,Default,Astra User2,0,0,0,,Why will this not stay in place?
Dialogue: 0,0:11:19.35,0:11:22.97,Default,Astra User2,0,0,0,,And in a near instant, Google gives me an AI overview.
Dialogue: 0,0:11:23.82,0:11:38.85,Default,host,0,0,0,,那对于我这种3C杀手、经常弄坏各种电器的人来说，简直太期待这个多模态的搜索功能了。而且多模态模型Gemini的强大搜索和推理能力还能够做更多的事情，那么这正好也是我的痛点啊。
Dialogue: 0,0:11:39.12,0:12:01.18,Default,host,0,0,0,,那比如说，谷歌CEO Pichai在现场就演示，Gemini可以在谷歌相册Google Photos里面进行更多的相关搜索。比如说，通过名为Ask Photos with Gemini的新功能啊，让Gemini找到用户想要的车牌账号。实话告诉大家，我就是那个记不住我家车牌号的人。如果大家也有人这样，可以在弹幕上让我知道，我不是一个人。
Dialogue: 0,0:12:01.35,0:12:20.14,Default,host,0,0,0,,所以呢，谷歌Gemini可以在用户的相册里面去搜索，找到相应的信息和对应的照片来，比如说，获取照片中拍到的车牌照号码，那么这个功能对我来说啊，真的是非常的期待了，以及任何可以帮我寻找以往照片、文件中信息的功能，我觉得都会解决很多的痛点。
Dialogue: 0,0:12:20.40,0:12:27.95,Default,host,0,0,0,,还有一个对我来说很大帮助的，是谷歌AI呢，将会结合到谷歌的所有Workspace当中，俗称谷歌全家桶。
Dialogue: 0,0:12:28.33,0:12:38.47,Default,host,0,0,0,,也就是说，在Gemini的加持之下，谷歌Workspace包括了Gmail、谷歌Docs、谷歌Drive、谷歌Calendar、谷歌Meet等等都可以打通，可以在这里进行跨文档搜索。
Dialogue: 0,0:12:38.78,0:12:54.34,Default,host,0,0,0,,比如说，你在邮箱里面收到了一张发票，那么就可以直通过Gemini，把这张发票整理到网盘Google Drive和表格Google Sheet当中，还可以在邮件中搜索、读取信息和亮点，归纳总结。那么这些功能啊，都会在今年稍后推出。
Dialogue: 0,0:12:55.12,0:13:14.50,Default,host,0,0,0,,另外呢，谷歌还发布了一系列其他的模型更新，包括了画图的Imagen 3、音乐的Music AI Sandbox、还有生成视频的Veo，还有有史以来最长上下文窗口——200万token的Gemini 1.5 Pro，还有呢，Gemini app以及谷歌的自研芯片第6代TPU等等等等。
Dialogue: 0,0:13:14.75,0:13:32.81,Default,host,0,0,0,,那因为其中的细节和产品太多了，这个视频我们就不一一地复述了。如果感兴趣的小伙伴，可以去看看谷歌的两小时发布会全程。那看到这里，你可能会问了，在OpenAI之后发布了这一系列重磅更新的谷歌，两个对手这一轮的发布，谁是赢家呢？
Dialogue: 0,0:13:36.33,0:13:57.15,Default,host,0,0,0,,那在两场发布会之后呢，我看到不少人，在对比OpenAI和谷歌的产品发布啊。那我们从公司的策略层来解读一下。首先，OpenAI比谷歌IO早一天发布了春季更新，而且非常临时，很难猜测是不是故意抢在谷歌前面的。发布时长也只持续了26分钟，非常地聚焦在GPT-4o的这一个产品上。
Dialogue: 0,0:13:57.46,0:14:25.80,Default,host,0,0,0,,虽然外界对GPT-4o的评价，没有说像当时发布ChatGPT的时候那么轰动、那么惊喜啊，但是不得不说，业界的很多人还是觉得，是一个非常重要的里程碑的。虽然多模态的这些功能是去年行业内的共识啊，OpenAI会在2024年做出来并且发布，并没有那么多的惊喜或者创新，但是呢，它确实是实现了大家期待中早晚会实现的AI更新，也是非常的有意义的，并且也是正确的发展道路。
Dialogue: 0,0:14:25.83,0:14:51.01,Default,Howie Xu,0,0,0,,我觉得是，因为，嗯，一个技术不能被用起来，这个技术背后再怎么样也没啥意思。OpenAI这个，嗯，GPT-4这个模，模型出来，也能够做些Translation，就，就翻译啊什么，这并不是一个新东西。但如果没有实时效应，嗯，其实是很难落地。星期一，他那个宣布的东西让我感觉到，我有可能真的会去，有可能在…
Dialogue: 0,0:14:51.52,0:15:17.93,Default,Howie Xu,0,0,0,,比如，下次我，跟你一起去采访谁，或者跟谁讲话，你一个语言不懂、语言不通的人，我们真的可能用，就用打开我们的手机，来用translation。否则的话，就以前的，那个延迟这么慢，那个效果很不好，你都，你都不，你都不好意思拿出来用，对吧？呃，那为什么能够做到延，那个延迟性这么低，那被广泛认为的，就是因为他是做了一个比较，就像你说的，原生的…
Dialogue: 0,0:15:18.30,0:15:35.89,Default,Howie Xu,0,0,0,,Native的一个Multimodal，就说，我看了那个Demo，我的第一反应是说OK，以前他说的这些东西，我都是玩玩是可以的，但是，我是不会拎出来用的。但是，但是，他星期一给我的东西，我就觉得有可能我会拿来，就在实际的生活、工作的场景里面就可能用得到。
Dialogue: 0,0:15:36.06,0:15:51.79,Default,host,0,0,0,,如果光从语音助手的这个产品上来看，GPT 4o对打谷歌Project Astra。目前业内很多生意啊，仍然认为OpenAI是领先的。单从多模态模型上来说，GPT 4o呢，是OpenAI第一款完全原生的多模态模型。
Dialogue: 0,0:15:52.07,0:16:14.75,Default,host,0,0,0,,我们视频之前也有说到，它所有的多模态输入和输出呢，都是同一个神经网络处理的。那么，这使得GPT-4o能够接受文本、音频和图像的任意组合作为输入，并且生成文本、音频和图像的任意组合的输出，是所谓的“Multimodal In, Multimodal Out”。但是目前不少的业内人士认为，谷歌的Gemini目前并没有做到这个程度。
Dialogue: 0,0:16:14.75,0:16:38.69,Default,host,0,0,0,,那比如说，英伟达的高级科学家Jim Fan，他在LinkedIn上面就发表观点，认为说，谷歌是多模态作为输入，但是呢，并不是多模态作为输出，它是所谓的“Multimodal In, But Not Multimodal Out”。那么这意味着谷歌本次更新的视频、音乐等等模型，依然是独立于Gemini大模型的存在，只是输出的时候把所有模型给整合起来，拥有的多模态输出能力。
Dialogue: 0,0:16:39.01,0:17:04.23,Default,host,0,0,0,,所以Jim Fan认为啊，谷歌接下来整合所有的输入、输出模态，将是不可避免的未来发展。但是他还有一句评论，挺有意思的。Jim Fan说：“谷歌在这次发布会当中啊，做对的一件事是，他们终于认真努力地将AI集成到搜索框中了。谷歌最坚固的护城河是分销，Gemini不一定要成为最好的模型，才能够成为世界上被使用最多的模型。”
Dialogue: 0,0:17:04.73,0:17:23.72,Default,host,0,0,0,,那么，这也就是说啊，谷歌在整个生态中，只要顺畅地融入AI功能，让用户觉得能够解决问题、提高生活和工作效率，因为谷歌在搜索、邮箱、谷歌云等等上的种种积累和优势，那谷歌的分销优势依然能够保证，谷歌在AI时代中，立于不败之地。
Dialogue: 0,0:17:24.03,0:17:44.29,Default,host,0,0,0,,所以呢，按照这个逻辑来看，谷歌在这次发布会上，在全生态上，全面升级AI功能，其实是做到了。所以呢，就算OpenAI前一天强跑，这个，发布亮点的GPT-4o，谷歌整体上来看啊，这一局也不算输。那么第二天的股价，稳中上涨啊，也是印证了市场的看法。
Dialogue: 0,0:17:44.74,0:18:08.21,Default,Yusen Dai,0,0,0,,在OpenAI发布会之后，Google发布会之前，我跟一位Google的同学聊，啊，然后他提到一个观点还挺有意思的。他说：“一年以前，呃，OpenAI发GPT-4的时候，他们，有很多东西他们是不知道OpenAI怎么做到的，觉得他们好厉害，对吧。现在呢，OpenAI发布会发了之后，他们看到是说，哦，这个东西我们也知道怎么做，但我们可能还没有像他那样，做得那么好，或者那么ready去demo。
Dialogue: 0,0:18:08.73,0:18:16.28,Default,Yusen Dai,0,0,0,,所以我觉得，目前来看的话，属于他们肯定，在这上面是有一些，这个，经验的。所以我感觉就是，双方的绝对差距还是在缩小的。
Dialogue: 0,0:18:16.41,0:18:37.37,Default,Howie Xu,0,0,0,,相对来讲，emm，那个Google 注重的是，一个solution，就解决方案。嗯，那个OpenAI目前解，注重的更多的还是一个technology。它在technology上面，非常的惊艳，但你说它怎么去跟我们的、人的日常的，不管是生活、工作去结合起来？它没有那么多的人力，它也没这么多思考，而且这不是它的强项。
Dialogue: 0,0:18:37.82,0:18:56.06,Default,Howie Xu,0,0,0,,Google IO的那个发布，看上去可能，嗯，从某种，从某些角度来讲，喔唷，好像还没有那个前一天的，嗯， OpenAI 的东西那么惊艳。但实际上，我觉得，嗯，很惊艳。我觉得惊艳不只是说，是一个model的惊艳，model 只是一个维度，还有其他维度怎么跟我的…
Dialogue: 0,0:18:56.50,0:19:08.91,Default,Howie Xu,0,0,0,,生活、工作能够结合起来，比如说，跟我的手机结合起来。它一些 announcement 是这个，技术。所以说一个技术， AI 这件技术，我觉得今天，落地是一个很，很大的一个挑战或者说一件事情。
Dialogue: 0,0:19:09.12,0:19:26.14,Default,host,0,0,0,,所以呢，可以预期到，接下来多模态的继续整合和优化，以及呢，将 AI 功能整合到谷歌的各个产品线当中。以及呢，AI Agent 的引入，将会是谷歌发力的重点。除此之外，这两场发布会听下来，还让我非常感兴趣的一点啊，在于硬件。
Dialogue: 0,0:19:26.58,0:19:47.63,Default,host,0,0,0,,OpenAI 整个 demo 用的是苹果手机和苹果电脑，谷歌用的是安卓手机和硬件。同时呢，还在视频 demo 当中啊，非常有意思地提到了，一个谷歌内部类似谷歌眼镜一样的 prototype 原型设备。所以接下来，硬件和 AI 大模型的整合，也到了加入战场的时刻。而这个赛道的老大，苹果在干什么呢？
Dialogue: 0,0:19:50.82,0:20:17.58,Default,host,0,0,0,,虽然苹果公司在这轮硅谷科技巨头 AI 大战中，是迟迟没有发声，但最近有不少的舆论风向，稍微给我们勾勒出了苹果潜在的想法和布局啊。目前呢，市场都在等待 6 月 10 号举行的苹果 2024 年全球开发者大会 WWDC，预计呢，会在届时，宣布一系列在 AI 和硬件上面的产品发布，包括可能会和 OpenAI 合作，将 ChatGPT 整合到 iOS 18 操作系统。
Dialogue: 0,0:20:17.91,0:20:35.38,Default,host,0,0,0,,那么另外呢，外界期待苹果呢，会宣布利用大模型全面升级 Siri，给用户提供 AI 赋能的交互体验。还有呢，苹果如何将大模型塞进手机移动端的这个苹果全家桶，也是啊，马上会召开的苹果发布会当中，大家非常期待的最大看点了。
Dialogue: 0,0:20:35.72,0:20:50.45,Default,host,0,0,0,,那么今年早前，苹果是发布了一系列的论文，包括了，第一个手机端 UI 多模态大模型 Fara UI，还有今年 1 月份发布的，一篇将大模型塞进 iPhone 的关键性论文，使用有限的内存实现更快的 LLM 推理。
Dialogue: 0,0:20:50.61,0:21:17.76,Default,host,0,0,0,,还有这篇，苹果 Siri 团队在论文《利用大型语言模型进行设备指向性语音检测的多模态方法》中，讨论了去掉唤醒词的方法。同时呢 ，在今年3月份发布的另外一篇论文中啊，苹果首次披露一个具有高达300亿参数的多模态模型MM1。那么，这个多模态能力如果集成到iPhone上面，就能通过视觉、语音和文本等多种方式理解并且响应用户的需求。
Dialogue: 0,0:21:18.14,0:21:45.10,Default,host,0,0,0,,所以呢，综上所述，虽然最近两年来啊，苹果时常被人诟病在AI领域动作迟缓，但是，感觉呢，苹果是在等一个正确的时机来加入战局。它并没有落后，而是一直在等待。如今啊，多模态技术逐渐成熟，特别是，文字输入、语音和视觉的交互，和手机等硬件是天然的适配，OpenAI 和谷歌的AI多模态之战打响之际，也是苹果入局的时间了。
Dialogue: 0,0:21:45.51,0:22:08.22,Default,Yusen Dai,0,0,0,,如果你看互联网和移动互联网时代，其实它们在软件的渗透上，其实都要叠加一个硬件的渗透。大家要买PC、大家要买手机，所以导致说，之前一个软件，它的渗透速度其实是相对比较慢。那为什么，比如说，GPT出来，就渗透到了这么多的用户？实际上，是因为它跑在一个已有的、比较成熟的硬件上。啊，所以，我觉得在目前来讲，AI落地肯定首选还是在手机上。
Dialogue: 0,0:22:08.35,0:22:25.81,Default,Yusen Dai,0,0,0,,我觉得，肯定是期待，像AI的这些模型，怎么样在苹果的生态系统中去落地。其实，说全新形态的硬件，我自己觉得可能性比较低。但是，在这个上面，有了最新的M4，对吧。但如果说，iPad这个上有这么强的芯片，你如果还是做原来的任务，是否又浪费了？你是不是用来干一些AI的任务呢？
Dialogue: 0,0:22:26.23,0:22:43.52,Default,Yusen Dai,0,0,0,,其实，我觉得，如果你去看两年前哈，两年前一些几百B、上百B的模型的能力，现在可能到了几十B，甚至在这个，十几B的，这个情况下，都能够去进行，呃，同样的性能。这其实是一个非常好的过程。
Dialogue: 0,0:22:43.73,0:23:01.99,Default,host,0,0,0,,而对于智能手机、智能手表，乃至于以后的VR和AR眼镜设备，更小的端模型，将是业界着重发力的重点。那么，在今年4月份啊，苹果宣布，在全球最大AI开源社区Huggingface上面，发布了全新的开源大型语言模型：OpenELM系列模型。
Dialogue: 0,0:23:02.24,0:23:19.22,Default,host,0,0,0,,包括了四个不同参数规模的模型：270M、450M、1.1B和3B。没错，最大的也只有30亿个参数。那么，对移动端小模型的布局有着明显的意图。而Howie Xu在采访中认为，端模型是人类应用AI发展的必然趋势。
Dialogue: 0,0:23:19.54,0:23:30.01,Default,Howie Xu,0,0,0,,过去一年，我们的大量的energy，大量的讨论都是在“越大越好”，对吧。但是万亿级的parameter不适合，呃，端的，不适合放在，那个，手机上面。
Dialogue: 0,0:23:30.19,0:23:52.77,Default,Howie Xu,0,0,0,,另外一个问题就是说，那个，不是万亿级的，对吧？千亿级的，或者说是，百亿级的参数，是否能够把模型做到足够好？现在我们看到的，很多的小的模型，可能是700，呃，700亿，呃，参数的。在一年之内，我们能够看到10亿这么一个参数的，一个model。
Dialogue: 0,0:23:53.15,0:24:09.09,Default,Howie Xu,0,0,0,,能够做到2022年11月30号，它第一次出来时候让大家惊艳的那个，相当于3.5的那个model的能力。我觉得是，呃，一个billion的，一个，一个billion的paramters应该能够做到。呃，如果能够，这个端上面能够…
Dialogue: 0,0:24:09.65,0:24:34.56,Default,Howie Xu,0,0,0,,run一个billion parameter的，呃，model，能够做到3.5，那就打开了很多的想象空间。然后，接下去会有更小的model，因为model总归是越小，对耗电啊、对各方面啊，都有，那个，都，都有很大好处。我觉得，甚至于是sub，嗯，one billion，呃，parameter会更好。从privacy的角度，从耗电的角度，从各方面角度，我觉得，小模型是必须的。
Dialogue: 0,0:24:34.73,0:24:53.41,Default,host,0,0,0,,那视频的最后啊，我们来总结一下，OpenAI和谷歌的这两场发布会。AI多模态战打响了之后呢，在更多更广的应用上，我们看到了 AI 杀手级应用的曙光，有了更落地、更切实的可用性。那么这将重塑人类和 AI，以及电子设备的交互方式。
Dialogue: 0,0:24:53.85,0:25:13.56,Default,host,0,0,0,,此外呢，虽然OpenAI和谷歌表面上m刀光剑影，但是两家公司的策略目标呢是有一些些区别的。前者是一路涌向前，目标Scaling Law和AGI；那么后者呢，更加注重自家的生态和应用落地，来捍卫商业营收和市场分销护城河。可能模型是不是最好的并没有那么重要。
Dialogue: 0,0:25:14.14,0:25:28.83,Default,host,0,0,0,,所以目前的多模态出战啊，OpenAI虽然赢了，但是谷歌也没有输。那么，在硬件端，各类硬件与 AI 的结合将带来巨大的新机会。而大模型瘦身进手机只是开始，打造应用体验才是关键所在。
Dialogue: 0,0:25:29.08,0:25:50.91,Default,host,0,0,0,,此外，让人惊喜的是，谷歌demo最后展示的AR眼镜与AI的结合，这给AR智能眼镜这个起起伏伏了好几个周期的产品，带来了新的曙光和希望。除了谷歌多年的AR经验，Meta在AR硬件上的布局与苹果在Vision Pro以及自家AR团队的未来策略，都有可能成为下一场科技硬件巨头们比拼的新战场。
Dialogue: 0,0:25:51.29,0:26:19.67,Default,host,0,0,0,,哦对了，不要忘记微软这家与OpenAI深度绑定的巨头，它并没有将全部鸡蛋都放在OpenAI的篮子当中。微软目前在AI布局上的优势，加上呢，在软硬件上都有多年的经验和布局，那么最近呢，还收编了之前主打情感陪伴大模型公司Inflection的大部分AI顶级人才，发布了自己的大模型MyOne。所以，我们很兴奋地能够感觉到，甚至是AI的第二轮多模态战役打响了。
Dialogue: 0,0:26:20.03,0:26:34.50,Default,host,0,0,0,,越来越多的科技巨头入局，并且战术和方向呢，也越发清晰，带来的也是 AI 应用的潜在落地与爆发。 这场战斗，硅谷 101 在最前线，所以别忘了关注我们的账号，别错过更新。那我们就拭目以待了。
Dialogue: 0,0:26:34.50,0:27:04.40,Default,host,0,0,0,,以上呢，就是这期视频的全部内容了。抱歉我们这一期的背景有点不硅谷哈，因为我在希腊休假的时候，被临时抓来加入的这一期。在这么平静、原始的海岛上，看 OpenAI 和谷歌的直播，并且写了这篇 AI 的稿件，感觉非常的有趣和穿越。同时，非常感谢大家！欢迎大家留言、点赞、转发和评论，你们的支持是我们硅谷 101 做好深度科技内容和商业内容的最佳动力。那我们就下一个视频，硅谷再见了！Bye。
